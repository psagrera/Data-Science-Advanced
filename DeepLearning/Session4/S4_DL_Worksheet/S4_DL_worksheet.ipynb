{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"mioti.png\" style=\"height: 100px\">\n",
    "<center style=\"color:#888\">Módulo Data Science in IoT<br/>Asignatura Deep Learning</center>\n",
    "\n",
    "# Worksheet S4: Redes Neuronales Convolucionales en Keras (CNNs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivos\n",
    "\n",
    "El objetivo de este worksheet es comprender las características principales de las redes convolucionales así como su implementación en Keras a través de un ejemplo de reconocimiento de objetos en imágenes.\n",
    "\n",
    "El objetivo de nuestra red neuronal convolucional será identificar y clasificar objetos específicos dentro de una imagen. Los píxeles de la imagen serán nuestros datos de entrada y las etiquetas serán el tipo de objeto que queremos reconocer.\n",
    "\n",
    "Como ya sabemos cómo funcionan las redes neuronales, podemos pasar directamente a analizar las particularidades de las redes convolucionales:\n",
    "- Imagenes como dato de entrada\n",
    "- Capa convolucional\n",
    "- Capa de muestreo o pooling\n",
    "- Arquitectura de una CNN\n",
    "\n",
    "\n",
    "## Introducción\n",
    "\n",
    "Las redes neuronales convolucionales están presentes en una gran cantidad de algoritmos del estado del arte en Machine Learning. Son un tipo de red neuronal en el que las neuronas corresponden a campos receptivos, de forma muy parecida a las neuronas en la corteza visual primaria de un cerebro humano. Desde un punto de vista técnico, son una variación de las redes neuronales profundas (perceptrón multicapa), donde las neuronas se aplican a matrices bidimensionales y algunas de sus neuronas comparten pesos entre sí (reduciendo así también el número de parámetros del modelo). Por este motivo, son tremendamente efectivas en tareas de visión artificial, como la clasificación y/o segmentación de imagenes o videos.\n",
    "\n",
    "En la actualidad, las redes neuronales convolucionales pueden trabajar con arrays de 1D (señales o secuencias), 2D (imágenes) o 3D (video). Son el estado del arte en innumerables problemas como el reconocimiento de objetos o la transcripción de escritura manuscrita.\n",
    "\n",
    "## Imagen como dato de entrada\n",
    "\n",
    "Hasta ahora, hemos trabajado con datos de entrada de 1 o 2 dimensiones. Ahora vamos a empezar a trabajar con imágenes, que normalmente tienen 3 dimensiones:\n",
    "- Longitud de la imagen.\n",
    "- Altura de la imagen.\n",
    "- Número de canales (colores).\n",
    "\n",
    "La novedad ahora es el número de canales de color. Esto representa la profundidad de la imagen, como podemos ver en la imagen de abajo, y está relacionada con los colores que se utilizan. Normalmente, las imagenes a color están compuestas de los 3 canales rgb: red, green, blue. Por tanto, para cada píxel tenemos 3 valores numéricos en el rango 0 a 255 que se corresponden con la intensidad de cada uno de estos 3 colores. \n",
    "\n",
    "<img src=\"imageDepth.png\" style=\"height: 400px\">\n",
    "\n",
    "## Red Neuronal Convolucional (CNN)\n",
    "\n",
    "Cada red convolucional está formada por una o varias capas convolucionales. Estas capas son muy diferentes de las capas dense o perceptrón que hemos visto hasta ahora. Las capas convolucionales se diseñaron originalmente para encontrar patrones en imágenes con el objetivo de clasificar imágenes o partes de imágenes. Espera... las DNNs también estaban diseñadas para encontrar patrones en nuestros datos de entrada, ¿No?.\n",
    "\n",
    "La diferencias fundamental es cómo abordan este problema. La red DNN busca patrones de forma global mientras que la red convolucional busca patrones de forma local. Cada neurona de la red DNN está conectada a toda la capa anterior y, por tanto, busca información de forma global. Las capas convolucionales no van a estar conectadas de esta forma, si no que van a buscar patrones en partes concretas de la capa anterior.\n",
    "\n",
    "Vamos a ver un ejemplo: Supongamos que partiendo de esta imagen, nuestro objetivo es determinar si es una foto de un gato o no lo es.\n",
    "\n",
    "<img src=\"coco.png\" style=\"height: 600px\">\n",
    "\n",
    "**Red DNN:** Una red DNN consideraría la imagen completa. Recibiría todos los pixels columna a columna (o fila a fila) y utilizaría toda la información para generar la salida.\n",
    "\n",
    "**Red CNN:** Una red convolucional puede mirar a partes específicas de la imagen. Imaginad que la red analizase las partes señaladas en la imagen de abajo para detectar patrones.\n",
    "\n",
    "<img src=\"coco2.png\" style=\"height: 600px\">\n",
    "\n",
    "\n",
    "## Partes de una Red Neuronal Convolucional\n",
    "\n",
    "A continuación vamos a desgranar las distintas partes de una red neuronal convolucional para entender su funcionamiento.\n",
    "\n",
    "En primer lugar, debemos saber que a lo que llamamos una capa de una CNN suele estar formado por dos sub-capas que son la sub-capa convolucional y la sub-capa de pooling o subsampling.\n",
    "\n",
    "### Capa convolucional\n",
    "\n",
    "La capa convolucional puede entenderse como un extractor de características. Vamos a ver un ejemplo:\n",
    "\n",
    "<img src=\"cnn_01.png\" style=\"height: 400px\">\n",
    "\n",
    "#### Campo Receptivo (Receptive Field)\n",
    "\n",
    "Cada capa convolucional va a examinar un campo receptivo, que es un tensor en 3 dimensiones (ancho, alto y profundidad). Lo bloques más utilizados son de 3x3 o 5x5 píxeles. Una red CNN toma un campo receptivo como entrada y devuelve un mapa de características (feature map) que representa la presencia o no de ciertos filtros específicos en el campo receptivo.\n",
    "\n",
    "#### Filtro y Mapa de Características (Filter and Feature Map)\n",
    "\n",
    "Un filtro es un patrón de m x n pixels que buscamos en una imagen. El número de filtros en una red convolucional representa el número de patrones que vamos a buscar y, por tanto, la profundidad de nuestro mapa de características. Si tenemos una capa convolucional con 32 filtros, buscaremos 32 filtros o patrones diferentes y tendremos un mapa de características de profundidad 32. Cada una de las capas será una matriz que contiene valores que indican la presencia o no del filtro que buscamos en dicha posición.\n",
    "\n",
    "Esto se ve muy bien en la siguiente ilustración del libro \"Deep Learning with Python\" de Francois Chollet:\n",
    "\n",
    "<img src=\"filter.png\" style=\"height: 400px\">\n",
    "\n",
    "Por tanto, la matriz de pesos contiene todos los filtros que deberemos aplicar.\n",
    "\n",
    "Suponiendo que nuestra capa convolucional tiene un sólo filtro, lo que hacemos es ir recorriendo nuestra imagen de entrada poco a poco (moviendo el filtro), de forma que el valor de la activación de cada unidad de nuestro mapa de caraterísticas (_feature map_) se extraerá mediante la aplicación de una función no lineal a la convolución de nuestro campo receptivo con nuestro filtro (pesos), produciendo un solo valor en el mapa de características por cada sub-región de la imagen de entrada a la que aplicamos el filtro.\n",
    "\n",
    "Como vemos en las imagenes siguientes, lo que vamos haciendo es simplemente mover este campo receptivo poco a poco por toda la imagen de entrada hasta que la hemos recorrido por completo, obteniendo un mapa de características.\n",
    "\n",
    "<img src=\"cnn_02.png\" style=\"height: 400px\">\n",
    "<img src=\"cnn_03.png\" style=\"height: 400px\">\n",
    "\n",
    "Como decíamos, nuestra sub-capa convolucional tendrá varios filtros, por lo que este proceso se repite para cada uno de ellos, obteniendo así varios mapas de características, y extrayendo por tanto distintas características de la imagen de entrada.\n",
    "\n",
    "<img src=\"cnn_04.png\" style=\"height: 400px\">\n",
    "\n",
    "#### Bordes y Padding\n",
    "\n",
    "Al deslizar un filtro de tamaño m x n por la imagen de entrada tenemos que decidir qué hacemos con los bordes de la misma. Una opción es empezar por el borde de arriba a la izquierda observando los primeros m x n píxeles, pero al recorrer la imagen de esta forma obtendremos un mapa de características más pequeño que la imagen original.\n",
    "\n",
    "En ocasiones, esto no es lo que buscamos y en estos casos se utiliza el padding. Básicamente, lo que hacemos es comenzar el análisis como si hubiese bordes imaginarios en la imagen que se rellenaran con ceros o replicando el pixel real más cercano.\n",
    "\n",
    "<img src=\"padding.png\" style=\"height: 400px\">\n",
    "\n",
    "#### Strides\n",
    "\n",
    "Hasta ahora, hemos considerado que los filtros vamos a moverlos de forma continua y de píxel en píxel. Sin embargo, muchas veces moveremos los filtros de 2 en 2 píxeles o más. El tamaño de stride representa cuantas filas o columnas movemos el filtro cada vez, es decir, el tamaño del paso de este movimiento.\n",
    "\n",
    "### Capa de agrupación (pooling)\n",
    "\n",
    "A cada capa convolucional le suele seguir una capa _pooling_, que se encarga de reducir la dimensionalidad de la imagen (mapa de características) extraída por la capa convolucional. Esto se hace para reducir el tiempo de procesado necesario y para obtener cierta invariabilidad a pequeñas rotaciones o traslaciones. \n",
    "\n",
    "Por tanto, la capa de pooling realiza una operación de agrupamiento (o resumen) recorriendo poco a poco la imagen de entrada (que es la salida de la capa convolucional). De forma similar a lo que hacía la capa anterior, irá mirando a un campo receptivo y realizando una operación sencilla, que suele ser una media de todos los valores o una selección del máximo. Una posibilidad sería un campo receptivo de 2x2 pixels y función max, con esta configuración esta capa iría procesando la imagen de entrada en regiones de 2x2 y seleccionando el máximo de cada una de estas regiones.\n",
    "\n",
    "<img src=\"cnn_05.png\" style=\"height: 400px\">\n",
    "\n",
    "\n",
    "La arquitectura más típica de una red convolucional consiste en una serie de módulos convolucionales que actúan como extractor de características. Cada uno de estos módulos está formado por una capa convolucional seguido de una capa pooling. A continuación, se utiliza una o varias capas _fully-connected_ (perceptrón o _dense_), que realiza la clasificación final. En un problema de clasificación multiclase, la última capa será de tipo Dense y tendrá tantas neuronas como clases tenga el problema, y la función de activación para esta última capa de salida será de tipo _softmax_.\n",
    "\n",
    "La función de activación softmax fuerza a que la suma de todas las salidas sea 1, de forma que las salidas puedan interpretarse como la probabilidad de la imagen de entrada de pertenecer a cada una de esas clases.\n",
    "\n",
    "En la siguiente imagen podemos ver una posible estructura para un problema de reconocimiento de idioma, cuya entrada es un espectrograma (una representación en tiempo-frecuencia) y hay 8 posibles idiomas.\n",
    "\n",
    "<img src=\"cnn_06.png\" style=\"height: 400px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNNs en Keras\n",
    "\n",
    "Ahora vamos a ver cómo se definen estas capas en Keras. Para ello, como siempre, comenzamos importando los paquetes que vamos a necesitar, y los datos correspondientes a Fashion MNIST como en las sesiones anteriores.\n",
    "\n",
    "Además, separaremos las primeras 10000 imágenes para validación como hicimos en sesiones previas, y extraeremos el número de clases de forma automática a partir de las etiquetas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %tensorflow_version 2.x  # sólo es necesaria en colab\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.random.seed(123)  # for reproducibility\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# Import Fashion MNIST data\n",
    "fashion_mnist = keras.datasets.fashion_mnist.load_data()\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist\n",
    "\n",
    "# Primeras 10000 imágenes, las utilizamos como validación\n",
    "X_valid = train_images[:10000]\n",
    "Y_valid = train_labels[:10000]\n",
    "\n",
    "X_train = train_images[10000:]\n",
    "Y_train = train_labels[10000:]\n",
    "\n",
    "X_test = test_images\n",
    "Y_test = test_labels\n",
    "\n",
    "n_classes = len(np.unique(Y_train))\n",
    "\n",
    "Y_train = keras.utils.to_categorical(Y_train, n_classes)\n",
    "Y_valid = keras.utils.to_categorical(Y_valid, n_classes)\n",
    "Y_test = keras.utils.to_categorical(Y_test, n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación vamos a ver cada una de las capas que formarán nuestra CNN:\n",
    "\n",
    "### Capa de entrada\n",
    "\n",
    "En primer lugar, necesitamos modificar el formato de entrada de los datos, ya que las capas convolucionales esperan tensores de 3 dimensiones:  ancho x alto x #canales.\n",
    "\n",
    "Estos tensores serán transformados a 4 dimensiones, añadiendo el número de muestras como primera dimensión en tiempo de ejecución (dependiendo del tamaño del mini-batch). Nuestros conjuntos de datos tendrán como primera dimensión el número de ejemplos o muestras de cada uno de nuestros subconjuntos definidos (train: 50000, valid: 10000, test: 10000).\n",
    "\n",
    "Como nuestras imágenes son de tamaño 28x28 píxeles en escala de grises (monocromáticas) el número de canales será 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "# Reshape input to [width, height, #channels]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolucional\n",
    "\n",
    "Utilizaremos la capa Conv2D de Keras para crear una capa convolucional para entradas de 2 dimensiones. \n",
    "\n",
    "Recibe como entrada (entre otros):\n",
    "- filters, que es el número de filtros que se va a utilizar (número de _feature maps_).\n",
    "- kernel\\_size, que es el tamaño de los filtros.\n",
    "- strides, que es el paso que se va a utilizar entre campo receptivo y campo receptivo.\n",
    "- padding, que es para el relleno a la entrada, nosotros utilizaremos 'same'.\n",
    "- activation, que es la función (transformación) que se va a aplicar después de la convolución.\n",
    "\n",
    "Por tanto, definimos nuestro modelo secuencial y la capa de entrada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "=================================================================\n",
      "Total params: 320\n",
      "Trainable params: 320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conv2D transformará nuestro tensor en 4 dimensiones añadiendo el número de muestras del batch, y la salida será otro tensor de 4 dimensiones.\n",
    "\n",
    "### Pooling\n",
    "\n",
    "Utilizaremos la capa MaxPooling2D. Esta capa escoge el máximo de los valores del campo receptivo que le indiquemos. En nuestro caso utilizaremos, por ejemplo un filtro de tamaño 2x2 con un paso entre muestreos de 2.\n",
    "\n",
    "MaxPooling2D(self, pool_size=(2, 2), strides=None, padding='valid', data_format=None, **kwargs)\n",
    "\n",
    "Recibe como entrada (entre otros):\n",
    "- pool\\_size, que es el tamaño del campo receptivo.\n",
    "- strides, que es el paso al hacer el barrido de la imagen de entrada\n",
    "- padding, que es para el relleno a la entrada, nosotros utilizaremos 'same'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "=================================================================\n",
      "Total params: 320\n",
      "Trainable params: 320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vemos, esta capa no tiene parámetros entrenables.\n",
    "\n",
    "\n",
    "### Dense\n",
    "\n",
    "Finalmente, utilizaremos una capa de tipo Dense. Para ello primero vamos a convertir nuestra imagen 2D (la salida de la capa de pooling anterior) en un vector, para ello utilizaremos la capa Flatten.\n",
    "\n",
    "A continuacion utilizamos la capa Dense, que recibe como entrada el número de unidades y la función de activación, la salida será del tamaño del número de unidades."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               802944    \n",
      "=================================================================\n",
      "Total params: 803,264\n",
      "Trainable params: 803,264\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capa de salida\n",
    "\n",
    "Finalmente, necesitamos una capa de salida, que será de tipo Dense y tendrá tantas neuronas como clases tiene nuestro problema.\n",
    "\n",
    "La función de activación será de tipo softmax, de esta forma la salida pueda interpretarse como probabilidades de pertenecer a cada una de las clases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               802944    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 804,554\n",
      "Trainable params: 804,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compilación, entrenamiento y evaluación del modelo:\n",
    "\n",
    "Una vez que hemos definido el modelo, como en las sesiones anteriores, el siguiente paso es compilarlo. Para ello, utilizamos la función de coste de entropía cruzada y el optimizador Adam, además de especificar la probabilidad de acierto como la métrica que utilizaremos para visualización del rendimiento en tiempo de entrenamiento.\n",
    "\n",
    "Tras esto, indicamos a la función de entrenamiento _fit_ que queremos entrenar con los datos de X\\_train y sus etiquetas Y\\_train, con un tamaño de batch (ejemplo, 128), durante un número dado de épocas (por ejemplo, 5) y le añadimos nuestro conjunto de validación (X\\_valid, Y\\_valid), para que nos muestre el rendimiento en dicho conjunto al finalizar cada época.\n",
    "\n",
    "Al finalizar el entrenamiento, comprobamos su rendimiento sobre los datos de test con la función _evaluate_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "390/391 [============================>.] - ETA: 0s - loss: 4.3759 - accuracy: 0.7869"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1323 test_function  *\n        return step_function(self, iterator)\n    /opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1314 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1307 run_step  **\n        outputs = model.test_step(data)\n    /opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1266 test_step\n        y_pred = self(x, training=False)\n    /opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/keras/engine/input_spec.py:200 assert_input_compatibility\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer sequential expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 28, 28, 1) dtype=uint8>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 10) dtype=float32>]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8a387bf303d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m               metrics=['accuracy'])\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m history = model.fit(X_train, Y_train, batch_size=128, epochs=5, \n\u001b[0m\u001b[1;32m      5\u001b[0m           verbose=1, validation_data=[X_valid, Y_valid])\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1212\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1213\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1214\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1215\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1216\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1487\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_r\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1489\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1490\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1491\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[0;32m--> 763\u001b[0;31m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    764\u001b[0m             *args, **kwds))\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3050\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3051\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[0;32m-> 3279\u001b[0;31m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[1;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1323 test_function  *\n        return step_function(self, iterator)\n    /opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1314 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1307 run_step  **\n        outputs = model.test_step(data)\n    /opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1266 test_step\n        y_pred = self(x, training=False)\n    /opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/keras/engine/base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /opt/anaconda3/envs/mioti_nlp/lib/python3.9/site-packages/tensorflow/python/keras/engine/input_spec.py:200 assert_input_compatibility\n        raise ValueError('Layer ' + layer_name + ' expects ' +\n\n    ValueError: Layer sequential expects 1 input(s), but it received 2 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 28, 28, 1) dtype=uint8>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 10) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=128, epochs=5, \n",
    "          verbose=1, validation_data=[X_valid, Y_valid])\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualización del entrenamiento\n",
    "\n",
    "Otra de las utilidades que nos ofrece Keras es la posibilidad de acceder al histórico del entrenamiento de un modelo para poder ver las curvas de entrenamiento.\n",
    "\n",
    "Para acceder a este histórico, tan sólo tenemos que guardar la salida de la llamada a model.fit.\n",
    "\n",
    "El código de una función para imprimir las curvas de aprendizaje por pantalla se muestra a continuación.\n",
    "\n",
    "Ejecútalo y contesta las preguntas de la siguiente celda para el primer modelo y el segundo (una o dos capas convolucionales +  maxpooling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "############## Si al ejecutar el Kernel se bloquea, \n",
    "############## utiliza estas líneas para permitir la \n",
    "############## duplicación de librerías\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "##############\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "def plotCurves(history,epochs):\n",
    "\n",
    "    plt.figure(0)\n",
    "    plt.plot(history.history['accuracy'],'r')\n",
    "    plt.plot(history.history['val_accuracy'],'g')\n",
    "    plt.xticks(np.arange(0, epochs, 2.0))\n",
    "    plt.rcParams['figure.figsize'] = (8, 6)\n",
    "    plt.xlabel(\"Num of Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Training Accuracy vs Validation Accuracy\")\n",
    "    plt.legend(['train','validation'])\n",
    " \n",
    " \n",
    "    plt.figure(1)\n",
    "    plt.plot(history.history['loss'],'r')\n",
    "    plt.plot(history.history['val_loss'],'g')\n",
    "    plt.xticks(np.arange(0, epochs, 2.0))\n",
    "    plt.rcParams['figure.figsize'] = (8, 6)\n",
    "    plt.xlabel(\"Num of Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss vs Validation Loss\")\n",
    "    plt.legend(['train','validation'])\n",
    " \n",
    " \n",
    "    plt.show()\n",
    "    \n",
    "plotCurves(history,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumen del código utilizado\n",
    "\n",
    "A modo de resumen, se muestra a continuación el código que se ha utilizado, aunque se ha añadido otra capa convolucional para que lo tengáis de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 14, 14, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1568)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               200832    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 202,442\n",
      "Trainable params: 202,442\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/5\n",
      "50000/50000 [==============================] - 20s 391us/sample - loss: 1.6849 - accuracy: 0.7910 - val_loss: 0.5239 - val_accuracy: 0.8390\n",
      "Epoch 2/5\n",
      "50000/50000 [==============================] - 18s 363us/sample - loss: 0.4336 - accuracy: 0.8605 - val_loss: 0.4126 - val_accuracy: 0.8648\n",
      "Epoch 3/5\n",
      "50000/50000 [==============================] - 17s 349us/sample - loss: 0.3372 - accuracy: 0.8831 - val_loss: 0.3723 - val_accuracy: 0.8759\n",
      "Epoch 4/5\n",
      "50000/50000 [==============================] - 18s 366us/sample - loss: 0.2887 - accuracy: 0.8963 - val_loss: 0.3569 - val_accuracy: 0.8764\n",
      "Epoch 5/5\n",
      "50000/50000 [==============================] - 18s 351us/sample - loss: 0.2556 - accuracy: 0.9059 - val_loss: 0.3453 - val_accuracy: 0.8849\n",
      "[0.3632947387456894, 0.8796]\n"
     ]
    }
   ],
   "source": [
    "# %tensorflow_version 2.x  # sólo es necesaria en colab\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "np.random.seed(123)  # for reproducibility\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "# Import Fashion MNIST data\n",
    "fashion_mnist = keras.datasets.fashion_mnist.load_data()\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist\n",
    "\n",
    "# Primeras 10000 imágenes, las utilizamos como validación\n",
    "X_valid = train_images[:10000]\n",
    "Y_valid = train_labels[:10000]\n",
    "\n",
    "X_train = train_images[10000:]\n",
    "Y_train = train_labels[10000:]\n",
    "\n",
    "X_test = test_images\n",
    "Y_test = test_labels\n",
    "\n",
    "n_classes = len(np.unique(Y_train))\n",
    "\n",
    "# Reshape input to [width, height, #channels]\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_valid = X_valid.reshape(X_valid.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "\n",
    "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
    "Y_train = keras.utils.to_categorical(Y_train, n_classes)\n",
    "Y_valid = keras.utils.to_categorical(Y_valid, n_classes)\n",
    "Y_test = keras.utils.to_categorical(Y_test, n_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3), strides=(2,2), padding='same', activation='relu', input_shape=(28,28,1)))\n",
    "#model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "model.add(Conv2D(32, kernel_size=(3,3), padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train, batch_size=128, epochs=5, \n",
    "          verbose=1, validation_data=[X_valid, Y_valid])\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minireto\n",
    "\n",
    "Ahora os toca a vosotros:\n",
    "\n",
    "Jugad con el número de capas y la configuración de la red neuronal para mejorar el resultado conseguido.\n",
    "\n",
    "¿Se os ocurre alguna otra forma de mejorar el resultado con lo que hemos visto hasta ahora?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
